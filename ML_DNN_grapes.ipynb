{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Deep Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load and parse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset, 130 sample X dimension (130, 600), Y dimension (130,1)\n",
    "df = pd.read_csv('test.csv')\n",
    "dataset = df.values\n",
    "# split into input (X) and output (Y) variables, and training/test data\n",
    "X = dataset[:, 1:601]\n",
    "Y = dataset[:, 602]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n",
    "print (X_train.shape,Y_train.shape,Y_test.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Build Deep NN and implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create neep NN model\n",
    "model = Sequential([Dense(32, activation='relu', input_shape = (600, )), \n",
    "                   Dense(32,  activation='relu'),\n",
    "                   Dense(1, activation='sigmoid'),\n",
    "                   ])\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "# fitting model and evaluation\n",
    "hist = model.fit(X_train, Y_train, batch_size = 16, epochs = 500, \n",
    "                validation_data =(X_test, Y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Plot loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "#accuracy\n",
    "plt.plot(hist.history['acc'])\n",
    "plt.plot(hist.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PLSR, Ridge Regress, RF, SVM and ELR\n",
    "\n",
    "## 2.1 Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries for above regression methos \n",
    "\n",
    "from sys import stdout\n",
    "from scipy.signal import savgol_filter #for derivative calculaiton\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn.cross_decomposition\n",
    "from sklearn.cross_decomposition import  PLSRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 PLSR\n",
    "### 2.2.1 Load and parse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original raw CA data (130 samples)\n",
    "\n",
    "data = pd.read_csv('test.csv')\n",
    "X = data.values[:, 1:601]\n",
    "Y = data['Gs']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Plotting spectral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = np.arange(400, 1000, 1)\n",
    "\n",
    "#plot reflectance spectra\n",
    "\n",
    "with plt.style.context(('ggplot')):\n",
    "    plt.plot(wl, X.T)\n",
    "    plt.xlabel('Wavelength (nm)')\n",
    "    plt.ylabel('Reflectance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Calculate 1st/2nd derivatives and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate first or second derivative (look at parameter values deriv)\n",
    "X2 = savgol_filter(X, 17, polyorder = 2,deriv=2)\n",
    "# Plot second derivative\n",
    "plt.figure(figsize=(8,4.5))\n",
    "with plt.style.context(('ggplot')):\n",
    "    plt.plot(wl, X2.T)\n",
    "    plt.xlabel('Wavelength (nm)')\n",
    "    plt.ylabel('D2 Absorbance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Function: PLSR optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://nirpyresearch.com/partial-least-squares-regression-python/\n",
    "def optimise_pls_cv(X, y, n_comp, plot_components=True):\n",
    "    '''Run PLS including a variable number of components, up to n_comp,\n",
    "       and calculate MSE '''\n",
    "    mse = []\n",
    "    component = np.arange(1, n_comp)\n",
    "    for i in component:\n",
    "        pls = PLSRegression(n_components=i)\n",
    "        # Cross-validation\n",
    "        y_cv = cross_val_predict(pls, X, y, cv=10)\n",
    "        mse.append(mean_squared_error(y, y_cv))\n",
    "        comp = 100*(i+1)/40\n",
    "        # Trick to update status on the same line\n",
    "        stdout.write(\"\\r%d%% completed\" % comp)\n",
    "        stdout.flush()\n",
    "    stdout.write(\"\\n\")\n",
    "    # Calculate and print the position of minimum in MSE\n",
    "    msemin = np.argmin(mse)\n",
    "    print(\"Suggested number of components: \", msemin+1)\n",
    "    stdout.write(\"\\n\")\n",
    "    if plot_components is True:\n",
    "        with plt.style.context(('ggplot')):\n",
    "            plt.plot(component, np.array(mse), '-v', color = 'blue', mfc='blue')\n",
    "            plt.plot(component[msemin], np.array(mse)[msemin], 'P', ms=10, mfc='red')\n",
    "            plt.xlabel('Number of PLS components')\n",
    "            plt.ylabel('MSE')\n",
    "            plt.title('PLS')\n",
    "            plt.xlim(left=-1)\n",
    "        plt.show()\n",
    "    # Define PLS object with optimal number of components\n",
    "    pls_opt = PLSRegression(n_components=msemin+1)\n",
    "    # Fir to the entire dataset\n",
    "    pls_opt.fit(X, y)\n",
    "    y_c = pls_opt.predict(X)\n",
    "    # Cross-validation\n",
    "    y_cv = cross_val_predict(pls_opt, X, y, cv=10)\n",
    "    # Calculate scores for calibration and cross-validation\n",
    "    score_c = r2_score(y, y_c)\n",
    "    score_cv = r2_score(y, y_cv)\n",
    "    # Calculate mean squared error for calibration and cross validation\n",
    "    mse_c = mean_squared_error(y, y_c)\n",
    "    mse_cv = mean_squared_error(y, y_cv)\n",
    "    print('R2 calib: %5.3f'  % score_c)\n",
    "    print('R2 CV: %5.3f'  % score_cv)\n",
    "    print('MSE calib: %5.3f' % mse_c)\n",
    "    print('MSE CV: %5.3f' % mse_cv)\n",
    "    # Plot regression and figures of merit\n",
    "    rangey = max(y) - min(y)\n",
    "    rangex = max(y_c) - min(y_c)\n",
    "    # Fit a line to the CV vs response\n",
    "    z = np.polyfit(y, y_c, 1)\n",
    "    with plt.style.context(('ggplot')):\n",
    "        fig, ax = plt.subplots(figsize=(9, 5))\n",
    "        ax.scatter(y_c, y, c='red', edgecolors='k')\n",
    "        #Plot the best fit line\n",
    "        ax.plot(np.polyval(z,y), y, c='blue', linewidth=1)\n",
    "        #Plot the ideal 1:1 line\n",
    "        ax.plot(y, y, color='green', linewidth=1)\n",
    "        plt.title('$R^{2}$ (CV): '+str(score_cv))\n",
    "        plt.xlabel('Predicted $^{\\circ}$Brix')\n",
    "        plt.ylabel('Measured $^{\\circ}$Brix')\n",
    "        plt.show()\n",
    "return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 Fitt the best PLSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimise_pls_cv(X,Y, 40, plot_components=True)\n",
    "\n",
    "pls = PLSRegression(n_components=7)\n",
    "pls = pls.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.6 Fuction: VIP calcuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIP calculation\n",
    "# sources :(Used)https://www.researchgate.net/post/How_can_I_compute_Variable_Importance_in_Projection_VIP_in_Partial_Least_Squares_PLS\n",
    "# (possible) https://github.com/scikit-learn/scikit-learn/issues/7050\n",
    "\n",
    "def _calculate_vips(model):\n",
    "    t = model.x_scores_\n",
    "    w = model.x_weights_\n",
    "    q = model.y_loadings_\n",
    "    p, h = w.shape\n",
    "    vips = np.zeros((p,))\n",
    "    s = np.diag(np.matmul(np.matmul(np.matmul(t.T,t),q.T), q)).reshape(h, -1)\n",
    "    total_s = np.sum(s)\n",
    "    for i in range(p):\n",
    "        weight = np.array([ (w[i,j] / np.linalg.norm(w[:,j]))**2 for j in range(h) ])\n",
    "        vips[i] = np.sqrt(p*(np.matmul(s.T, weight))/total_s)\n",
    "    return vips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vip calculationa and plotting\n",
    "\n",
    "vip = _calculate_vips(pls)\n",
    "plt.figure(figsize=(8,4.5))\n",
    "with plt.style.context(('ggplot')):\n",
    "    plt.plot(wl, vip.T)\n",
    "    plt.xlabel('Wavelength (nm)')\n",
    "    plt.ylabel('VIP')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Ridge regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Define the parameters and their range\n",
    "parameters = {'alpha':np.logspace(-4, -3.5, 50)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a Grid search, using R^2 as the metric to optimize alpha\n",
    "ridge = GridSearchCV(linear_model.Ridge(), parameters, scoring ='r2', cv = 10)\n",
    "\n",
    "#fit to the data\n",
    "ridge.fit(X, Y)\n",
    "\n",
    "#Get the optimised value of alpha\n",
    "print('Best parameter alpha = ', ridge.best_params_['alpha'])\n",
    "print('R2 calibration: %5.3f'  % ridge.score(X,Y))\n",
    "# Run a ridge regression with the optimised value\n",
    "ridge1 = linear_model.Ridge(alpha=ridge.best_params_['alpha'])\n",
    "y_cv = cross_val_predict(ridge1, X, Y, cv=10)\n",
    "# y_cv=predicted\n",
    "score_cv = r2_score(Y, y_cv)\n",
    "mse_cv = mean_squared_error(Y, y_cv)\n",
    "print('R2 CV (Ridge): %5.3f'  % score_cv)\n",
    "print('MSE CV (Ridge): %5.3f' % mse_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfr_model(X, y):\n",
    "# Perform Grid-Search\n",
    "    gsc = GridSearchCV(\n",
    "        estimator=RandomForestRegressor(),\n",
    "        param_grid={\n",
    "            'max_depth': range(3,7),\n",
    "            'n_estimators': (10, 50, 100, 1000),\n",
    "        },\n",
    "        cv=5, scoring='neg_mean_squared_error', verbose=0,                         n_jobs=-1)\n",
    "    \n",
    "    grid_result = gsc.fit(X, y)\n",
    "    best_params = grid_result.best_params_\n",
    "    \n",
    "    rfr = RandomForestRegressor(max_depth=best_params[\"max_depth\"], n_estimators=best_params[\"n_estimators\"],                               random_state=False, verbose=False)\n",
    "# Perform K-Fold CV\n",
    "    scores = cross_val_score(rfr, X, y, cv=10, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rfr = rfr_model(X, Y)\n",
    "score = cross_val_score(rfr, X, Y, cv=10, scoring='neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicitons = cross_val_predict(rfr, X, Y, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 RF\n",
    "\n",
    "### 2.4.1 Select parameter and fit the best RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state = 42)\n",
    "\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit RF and pring the best parameters\n",
    "grid_search.fit(X_train, Y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Function: Evaluate RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, X_test, Y_test):\n",
    "    predictions = model.predict(X_test)\n",
    "    errors = abs(predictions - Y_test)\n",
    "    mape = 100 * np.mean(errors / Y_test)\n",
    "    accuracy = 100 - mape\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid = grid_search.best_estimator_\n",
    "grid_accuracy = evaluate(best_grid, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 SVM\n",
    "\n",
    "### 2.5.1 Select paramters and fit the best SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "parameters = {'kernel': ('linear', 'rbf','poly'), 'C':[1.5, 10],'gamma': [1e-7, 1e-4],'epsilon':[0.1,0.2,0.5,0.3]}\n",
    "svr = SVR()\n",
    "grid_search = GridSearchCV(svr, parameters, cv=5)\n",
    "grid_search.fit(X_train,Y_train)\n",
    "grid_search.best_params_\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "grid_accuracy = evaluate(best_grid, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_extensions.extre_leraning_mashines.elm import ELMRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 ELR\n",
    "\n",
    "### 2.6.1 Fuctions: Activation and Main ELR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def act_fun(x, actf):\n",
    "\n",
    "    # Calculate hidden neuron output matrix H\n",
    "    # select activation initially\n",
    "    if actf == 'tribas':\n",
    "        # triangular activation function\n",
    "        x = x.astype(float)\n",
    "        H = np.clip(1.0 - np.fabs(x), 0.0, 1.0)\n",
    "    elif actf == 'sig':\n",
    "        # sigmoid activation function\n",
    "        x = x.astype(float)\n",
    "        H = 1.0/(1.0 + np.exp(-x))\n",
    "    elif actf == 'hard_limit':\n",
    "        # hard limit actf function\n",
    "        x = x.astype(float)\n",
    "        H = np.array(x > 0.0, dtype=float)\n",
    "    elif actf == 'Gaussian':\n",
    "        # gaussian RBF\n",
    "        x = x.astype(float)\n",
    "        H = np.exp(-pow(x, 2.0))\n",
    "    elif actf == 'dual':\n",
    "        x = x.astype(float)\n",
    "        H = np.array(x > 0.0, dtype=float)*x + np.array(x <= 0.0, dtype=float)*np.tan(x)\n",
    "    return H\n",
    "\n",
    "def ELR_main(trainX, testX, trainY, testY, actf, NumberofHiddenNeurons):\n",
    "\n",
    "    NumberofInputNeurons = trainX.shape[1]\n",
    "\n",
    "    trainX = np.transpose(trainX)\n",
    "    trainY = np.transpose(trainY)\n",
    "    testX  = np.transpose(testX)\n",
    "    testY  = np.transpose(testY)\n",
    "    \n",
    "    # Random generate input weights InputWeight (w_i) and biases BiasofHiddenNeurons (b_i) of hidden neurons\n",
    "    np.random.seed(0)\n",
    "    InputWeight = np.random.random((NumberofHiddenNeurons,NumberofInputNeurons))*2-1\n",
    "    BiasofHiddenNeurons = np.random.random((NumberofHiddenNeurons,1))\n",
    "    tempH = np.dot(InputWeight, trainX)\n",
    "    # add bias \n",
    "    tempH = tempH + BiasofHiddenNeurons\n",
    "    H = act_fun(tempH, actf)\n",
    "    # Calculate output weights OutputWeight\n",
    "    OutputWeight = np.dot(np.linalg.pinv(np.transpose(H)), np.transpose(trainY))  # option 1\n",
    "                    \n",
    "    #Calculate the training pred\n",
    "    # Y_pred_train = np.transpose(np.dot(np.transpose(H), OutputWeight))\n",
    "    \n",
    "    #Calculate the output of testing input\n",
    "    tempH_test = np.dot(InputWeight, testX)\n",
    "    tempH_test = tempH_test + BiasofHiddenNeurons\n",
    "    H_test = act_fun(tempH_test, actf)\n",
    "    \n",
    "    # TY: the actual output of the testing data\n",
    "    y_pred = np.transpose(np.dot(np.transpose(H_test), OutputWeight))\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.2 Load and parse data, and fit ELR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('test.csv')\n",
    "dataset = df.values\n",
    "X = dataset[:, 1:601]\n",
    "Y = dataset[:, 602]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n",
    "\n",
    "#method = 'ELR'\n",
    "actf = 'hard_limit'\n",
    "\n",
    "R2_list = []\n",
    "\n",
    "for NumberofHiddenNeurons in range (1, 201):\n",
    "    y_pred = ELR_main(X_train, X_test, Y_train, Y_test, actf, NumberofHiddenNeurons)\n",
    "    R2 = r2_score(Y_test,y_pred)\n",
    "    R2_list.append(R2)\n",
    "    \n",
    "print (\"The list of R2 is:\", R2_list)\n",
    "\n",
    "maximum = R2_list[0]\n",
    "for i in range (0, 200):\n",
    "    if R2_list[i] > maximum:\n",
    "        maximum = R2_list[i]\n",
    "        bestNumberofNeurons = i+1\n",
    "print (\"The best number of neuron for the prediction is:\", bestNumberofNeurons)\n",
    "        \n",
    "theBestR2 = maximum\n",
    "print (\"The best R2 is:\", theBestR2)\n",
    "\n",
    "#NumberofHidddenNeurons = bestNumberofNeurons \n",
    "   \n",
    "y_pred = ELR_main(X_train, X_test, Y_train, Y_test, actf, bestNumberofNeurons)\n",
    "\n",
    "MSE = mean_squared_error(Y_test,y_pred)\n",
    "print(\"Mean Squared Error: %.2f\" % MSE)\n",
    "\n",
    "R2 = r2_score(Y_test,y_pred)\n",
    "print(\"R2:%.2f\" %R2)\n",
    "\n",
    "#MSE = mean_squared_error(testY,y_pred)\n",
    "#print(\"Mean Squared Error: %.2f\" % MSE)\n",
    "\n",
    "#R2 = r2_score(testY,y_pred)\n",
    "#print(\"R2:%.2f\" %R2)\n",
    "\n",
    "## Plot outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.3 Visualize ELR the best resutls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import text\n",
    "\n",
    "MSE = mean_squared_error(Y_test,y_pred)\n",
    "print(\"Mean Squared Error: %.2f\" % MSE)\n",
    "\n",
    "R2 = r2_score(Y_test,y_pred)\n",
    "print(\"R2:%.2f\" %R2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8, 6))\n",
    "ax.scatter(Y_test, y_pred, edgecolors=(0, 0, 0)) # testY is ground truth for testing data, y_pred is the predicted output\n",
    "ax.plot([Y_test.min(), Y_test.max()], [Y_test.min(), Y_test.max()], 'b--', lw=2)\n",
    "#text(int(Y_test.min())+1, int(Y_test.min())+24, 'R2=' + str(round(R2,2)), bbox=dict(facecolor='red', alpha=1))\n",
    "# print MSE on the figure\n",
    "#text(int(Y_test.min())+1, int(Y_test.min())+21, 'MSE=' + str(round(MSE,2)), bbox=dict(facecolor='red', alpha=0))\n",
    "ax.set_xlabel('Measured')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
